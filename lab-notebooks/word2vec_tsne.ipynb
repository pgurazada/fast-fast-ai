{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Word2Vec and t-SNE\n",
        "\n",
        "A question that might come up when working with text is: how do you turn text into numbers?\n",
        "\n",
        "In the past, common techniques included methods like one-hot vectors, in which we'd have a different number associated with each word, and then turn \"on\" the value at that index in a vector (making it 1) and setting all the rest to zero.\n",
        "\n",
        "For instance, if we have the sentence: \"I like dogs\", we'd have a 3-dimensional one-hot vector (3-dimensional because there are three words), so the word \"I\" might be `[1,0,0]`, the word \"like\" might be `[0,1,0]`, and \"dogs\" would be `[0,0,1]`.\n",
        "\n",
        "One-hot vectors worked well enough for some tasks but it's not a particularly rich or meaningful representation of text. The indices of these words are arbitrary and don't describe any relationship between them.\n",
        "\n",
        "[_Word embeddings_](http://arxiv.org/pdf/1301.3781.pdf) provide a meaningful representation of text. Word embeddings, called such because they involve embedding a word in some high-dimensional space, that is, they map a word to some vector, much like one-hot vectors. The difference is that word embeddings are learned for a particular task, so they end up being meaningful representations.\n",
        "\n",
        "For example, the relationships between words are meaningful (image from the [TensorFlow documentation]((https://www.tensorflow.org/versions/r0.9/tutorials/word2vec/index.html)):\n",
        "\n",
        "![Word embedding relationships](https://www.tensorflow.org/versions/r0.9/images/linear-relationships.png){:width=\"100%\"}\n",
        "\n",
        "A notable property that emerges is that vector arithmetic is also meaningful. Perhaps the most well-known example of this is:\n",
        "\n",
        "$$\n",
        "\\text{king} - \\text{man} + \\text{woman} = \\text{queen}\n",
        "$$\n",
        "\n",
        "([Chris Olah's piece on word embeddings](Deep Learning, NLP, and Representations) delves more into why this is.)\n",
        "\n",
        "So the positioning of these words in this space actually tells us something about how these words are used.\n",
        "\n",
        "This allows us to do things like find the most similar words by looking at the closest words. You can project the resulting embeddings down to 2D so that we can visualize them. We'll use t-SNE (\"t-Distributed Stochastic Neighbor Embedding\") for this, which is a dimensionality reduction method that works well for visualizing high-dimension data. We'll see that clusters of related words form in a way that a human would probably agree with. We couldn't do this with one-hot vectors - the distances between them are totally arbitrary and their proximity is essentially random.\n",
        "\n",
        "As mentioned earlier, these word embeddings are trained to help with a particular task, which is learned through a neural network. Two tasks developed for training embeddings is _CBOW_ (continuous bag of words) and _skip-grams_; together these methods of learning word embeddings are called \"Word2Vec\".\n",
        "\n",
        "For the CBOW task, we take the context words (the words around the target word) and give the target word. We want to predict whether or not the target word belongs to the context.\n",
        "\n",
        "The skip-grams is basically the inverse: we take the target word (the \"pivot\"), then give the context. We want to predict whether or not the context belongs to the word.\n",
        "\n",
        "They are quite similar but have different properties, e.g. CBOW works better on smaller datasets, where as skip-grams works better for larger ones. In any case, the idea with word embeddings is that they can be trained to help with any task.\n",
        "\n",
        "We're going to be using the skip-gram task here.\n",
        "\n",
        "## Corpus\n",
        "\n",
        "We need a reasonably-sized text corpus to learn from. Here we'll use State of the Union addresses retrieved from [The American Presidency Project](http://www.presidency.ucsb.edu/sou.php). These addresses tend to use similar patterns so we should be able to learn some decent word embeddings. Since the skip-gram task looks at context, texts that use words in a consistent way (i.e. in consistent contexts) we'll be able to learn better.\n",
        "\n",
        "[The corpus is available here](/guides/data/sotu.tar.gz). The texts were preprocessed a bit (mainly removing URL-encoded characters). The texts provided here are the processed versions (nb: this isn't the complete collection of texts but enough to work with here).\n",
        "\n",
        "## Skip-grams\n",
        "\n",
        "Before we go any further, let's get a bit more concrete about what the skip-gram task is.\n",
        "\n",
        "Let's consider the sentence \"I think cats are cool\".\n",
        "\n",
        "The skip-gram task is as follows:\n",
        "\n",
        "- We take a word, e.g. `'cats'`, which we'll represent as $w_i$. We feed this as input into our neural network.\n",
        "- We take the word's context, e.g. `['I', 'think', 'are', 'cool']`. We'll represent this as $\\{w_{i-2}, w_{i-1}, w_{i+1}, w_{i+2}\\}$ and we also feed this into our neural network.\n",
        "- Then we just want our network to predict (i.e. classify) whether or not $\\{w_{i-2}, w_{i-1}, w_{i+1}, w_{i+2}\\}$ is the true context of $w_i$.\n",
        "\n",
        "For this particular example we'd want the network to output 1 (i.e. yes, that is the true context).\n",
        "\n",
        "If we set $w_i$ to 'frogs', then we'd want the network output 0. In our one sentence corpus, `['I', 'think', 'are', 'cool']` is not the true context for 'frogs'. Sorry frogs üê∏.\n",
        "\n",
        "## Building the model\n",
        "\n",
        "We'll use `keras` to build the neural network that we'll use to learn the embeddings.\n",
        "\nFirst we'll import everything:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import codecs\n",
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers import Flatten, Activation, Dot, Input\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import skipgrams, make_sampling_table\n",
        "\nfrom glob import glob"
      ],
      "outputs": [],
      "execution_count": 30,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then load in our data. We're actually going to define a generator to load our data in on-demand; this way we'll avoid having all our data sitting around in memory when we don't need it."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "text_files = glob('../data/sotu/*.txt')\n",
        "\n",
        "def text_generator():\n",
        "    for path in text_files:\n",
        "        with codecs.open(path, 'r', 'utf-8') as f:\n",
        "            yield f.read()\n",
        "            \n",
        "len(text_files)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": [
              "84"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 9,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we go any further, we need to map the words in our corpus to numbers, so that we have a consistent way of referring to them. First we'll fit a tokenizer to the corpus:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# our corpus is small enough where we\n",
        "# don't need to worry about this, but good practice\n",
        "max_vocab_size = 50000\n",
        "\n",
        "# `filters` specify what characters to get rid of\n",
        "tokenizer = Tokenizer(num_words=max_vocab_size,\n",
        "                      filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_{|}~\\t\\n\\'`‚Äú‚Äù‚Äì')\n",
        "\n",
        "# fit the tokenizer\n",
        "tokenizer.fit_on_texts(text_generator())\n",
        "\n",
        "# we also want to keep track of the actual vocab size\n",
        "# we'll need this later\n",
        "# note: we add one because `0` is a reserved index in keras' tokenizer\n",
        "vocab_size = len(tokenizer.word_index) + 1"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.word_index"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 15,
          "data": {
            "text/plain": [
              "{'the': 1,\n",
              " 'of': 2,\n",
              " 'and': 3,\n",
              " 'to': 4,\n",
              " 'in': 5,\n",
              " 'we': 6,\n",
              " 'a': 7,\n",
              " 'our': 8,\n",
              " 'that': 9,\n",
              " 'for': 10,\n",
              " 'is': 11,\n",
              " 'i': 12,\n",
              " 'this': 13,\n",
              " 'will': 14,\n",
              " 'have': 15,\n",
              " 'be': 16,\n",
              " 'it': 17,\n",
              " 'are': 18,\n",
              " 'with': 19,\n",
              " 'on': 20,\n",
              " 'as': 21,\n",
              " 'by': 22,\n",
              " 's': 23,\n",
              " 'all': 24,\n",
              " 'not': 25,\n",
              " 'has': 26,\n",
              " 'their': 27,\n",
              " 'but': 28,\n",
              " 'must': 29,\n",
              " 'more': 30,\n",
              " 'can': 31,\n",
              " 'people': 32,\n",
              " 'world': 33,\n",
              " 'they': 34,\n",
              " 'from': 35,\n",
              " 'which': 36,\n",
              " 'new': 37,\n",
              " 'at': 38,\n",
              " 'year': 39,\n",
              " 'you': 40,\n",
              " 'government': 41,\n",
              " 'congress': 42,\n",
              " 'these': 43,\n",
              " 'an': 44,\n",
              " 'or': 45,\n",
              " 'us': 46,\n",
              " 'america': 47,\n",
              " 'now': 48,\n",
              " 'been': 49,\n",
              " 'years': 50,\n",
              " 'who': 51,\n",
              " 'american': 52,\n",
              " 'nation': 53,\n",
              " 'than': 54,\n",
              " 'do': 55,\n",
              " 'should': 56,\n",
              " 'so': 57,\n",
              " 'one': 58,\n",
              " 'war': 59,\n",
              " 'time': 60,\n",
              " 'its': 61,\n",
              " 'if': 62,\n",
              " 'those': 63,\n",
              " 'federal': 64,\n",
              " 'other': 65,\n",
              " 'there': 66,\n",
              " 'them': 67,\n",
              " 'make': 68,\n",
              " 'work': 69,\n",
              " 'was': 70,\n",
              " 'peace': 71,\n",
              " 'every': 72,\n",
              " 'states': 73,\n",
              " 'no': 74,\n",
              " 'national': 75,\n",
              " 'united': 76,\n",
              " 'my': 77,\n",
              " 'nations': 78,\n",
              " 'economic': 79,\n",
              " 'security': 80,\n",
              " 'program': 81,\n",
              " 'help': 82,\n",
              " 'also': 83,\n",
              " 'country': 84,\n",
              " 'great': 85,\n",
              " 'americans': 86,\n",
              " 'many': 87,\n",
              " 'would': 88,\n",
              " 'what': 89,\n",
              " 'only': 90,\n",
              " 'when': 91,\n",
              " 'most': 92,\n",
              " 'first': 93,\n",
              " 'need': 94,\n",
              " 'last': 95,\n",
              " 'free': 96,\n",
              " 'tax': 97,\n",
              " 'over': 98,\n",
              " 'let': 99,\n",
              " 'some': 100,\n",
              " 'own': 101,\n",
              " 'economy': 102,\n",
              " 'know': 103,\n",
              " 'because': 104,\n",
              " 'freedom': 105,\n",
              " 'future': 106,\n",
              " 'budget': 107,\n",
              " 'health': 108,\n",
              " 'million': 109,\n",
              " 'about': 110,\n",
              " 'out': 111,\n",
              " 'up': 112,\n",
              " 'through': 113,\n",
              " 'system': 114,\n",
              " 'made': 115,\n",
              " 'programs': 116,\n",
              " 'were': 117,\n",
              " 'into': 118,\n",
              " 'children': 119,\n",
              " 'billion': 120,\n",
              " 'any': 121,\n",
              " 'today': 122,\n",
              " 'well': 123,\n",
              " 'tonight': 124,\n",
              " 'power': 125,\n",
              " 'long': 126,\n",
              " 't': 127,\n",
              " 'continue': 128,\n",
              " 'together': 129,\n",
              " 'act': 130,\n",
              " 'good': 131,\n",
              " 'jobs': 132,\n",
              " 'defense': 133,\n",
              " 'just': 134,\n",
              " 'military': 135,\n",
              " 'such': 136,\n",
              " 'state': 137,\n",
              " 'support': 138,\n",
              " 'way': 139,\n",
              " 'here': 140,\n",
              " 'against': 141,\n",
              " '000': 142,\n",
              " 'public': 143,\n",
              " 'president': 144,\n",
              " 'administration': 145,\n",
              " 've': 146,\n",
              " 'policy': 147,\n",
              " 'forces': 148,\n",
              " 'progress': 149,\n",
              " 'give': 150,\n",
              " 'right': 151,\n",
              " 'shall': 152,\n",
              " 'better': 153,\n",
              " 'care': 154,\n",
              " 'energy': 155,\n",
              " 'hope': 156,\n",
              " 'under': 157,\n",
              " 'past': 158,\n",
              " 'take': 159,\n",
              " 'before': 160,\n",
              " 'next': 161,\n",
              " 'life': 162,\n",
              " 'increase': 163,\n",
              " 'still': 164,\n",
              " 'percent': 165,\n",
              " 'legislation': 166,\n",
              " 'men': 167,\n",
              " 'home': 168,\n",
              " 'had': 169,\n",
              " 'business': 170,\n",
              " 're': 171,\n",
              " 'where': 172,\n",
              " 'education': 173,\n",
              " 'dollars': 174,\n",
              " 'high': 175,\n",
              " 'cannot': 176,\n",
              " 'me': 177,\n",
              " 'meet': 178,\n",
              " 'growth': 179,\n",
              " 'both': 180,\n",
              " 'come': 181,\n",
              " 'provide': 182,\n",
              " 'without': 183,\n",
              " 'his': 184,\n",
              " 'citizens': 185,\n",
              " 'strength': 186,\n",
              " 'even': 187,\n",
              " 'want': 188,\n",
              " 'two': 189,\n",
              " 'use': 190,\n",
              " 'union': 191,\n",
              " 'believe': 192,\n",
              " 'development': 193,\n",
              " 'ask': 194,\n",
              " 'social': 195,\n",
              " 'problems': 196,\n",
              " 'your': 197,\n",
              " 'could': 198,\n",
              " 'during': 199,\n",
              " 'much': 200,\n",
              " 'he': 201,\n",
              " 'may': 202,\n",
              " 'trade': 203,\n",
              " 'history': 204,\n",
              " 'working': 205,\n",
              " 'families': 206,\n",
              " 'part': 207,\n",
              " 'very': 208,\n",
              " 'am': 209,\n",
              " 'action': 210,\n",
              " 'income': 211,\n",
              " 'effort': 212,\n",
              " 'private': 213,\n",
              " 'full': 214,\n",
              " 'international': 215,\n",
              " 'production': 216,\n",
              " '1': 217,\n",
              " 'again': 218,\n",
              " 'needs': 219,\n",
              " 'down': 220,\n",
              " '2': 221,\n",
              " 'plan': 222,\n",
              " 'law': 223,\n",
              " 'important': 224,\n",
              " 'resources': 225,\n",
              " 'opportunity': 226,\n",
              " 'same': 227,\n",
              " 'responsibility': 228,\n",
              " 'human': 229,\n",
              " 'too': 230,\n",
              " 'being': 231,\n",
              " 'each': 232,\n",
              " 'like': 233,\n",
              " 'strong': 234,\n",
              " 'fiscal': 235,\n",
              " 'between': 236,\n",
              " 'efforts': 237,\n",
              " 'get': 238,\n",
              " 'say': 239,\n",
              " 'best': 240,\n",
              " 'never': 241,\n",
              " 'soviet': 242,\n",
              " 'keep': 243,\n",
              " 'major': 244,\n",
              " 'end': 245,\n",
              " 'rights': 246,\n",
              " 'toward': 247,\n",
              " 'since': 248,\n",
              " 'ago': 249,\n",
              " 'back': 250,\n",
              " 'spending': 251,\n",
              " 'service': 252,\n",
              " 'foreign': 253,\n",
              " 'upon': 254,\n",
              " 'present': 255,\n",
              " 'ever': 256,\n",
              " 'go': 257,\n",
              " 'already': 258,\n",
              " 'day': 259,\n",
              " 'after': 260,\n",
              " 'workers': 261,\n",
              " 'interest': 262,\n",
              " 'far': 263,\n",
              " 'control': 264,\n",
              " 'nuclear': 265,\n",
              " 'local': 266,\n",
              " 'reform': 267,\n",
              " 'pay': 268,\n",
              " 'how': 269,\n",
              " 'labor': 270,\n",
              " 'welfare': 271,\n",
              " 'means': 272,\n",
              " 'while': 273,\n",
              " 'community': 274,\n",
              " 'see': 275,\n",
              " 'job': 276,\n",
              " 'force': 277,\n",
              " 'cut': 278,\n",
              " 'women': 279,\n",
              " 'another': 280,\n",
              " 'change': 281,\n",
              " 'cost': 282,\n",
              " 'greater': 283,\n",
              " 'among': 284,\n",
              " 'expenditures': 285,\n",
              " 'countries': 286,\n",
              " 'weapons': 287,\n",
              " 'inflation': 288,\n",
              " 'schools': 289,\n",
              " 'common': 290,\n",
              " 'areas': 291,\n",
              " 'necessary': 292,\n",
              " 'yet': 293,\n",
              " 'increased': 294,\n",
              " 'small': 295,\n",
              " 'forward': 296,\n",
              " 'build': 297,\n",
              " '5': 298,\n",
              " 'possible': 299,\n",
              " 'bring': 300,\n",
              " 'lives': 301,\n",
              " '3': 302,\n",
              " 'members': 303,\n",
              " 'century': 304,\n",
              " 'put': 305,\n",
              " 'protect': 306,\n",
              " 'family': 307,\n",
              " 'within': 308,\n",
              " 'further': 309,\n",
              " 'school': 310,\n",
              " 'housing': 311,\n",
              " 'special': 312,\n",
              " 'challenge': 313,\n",
              " 'cooperation': 314,\n",
              " 'goal': 315,\n",
              " 'millions': 316,\n",
              " 'europe': 317,\n",
              " 'reduce': 318,\n",
              " 'essential': 319,\n",
              " 'making': 320,\n",
              " 'propose': 321,\n",
              " 'taxes': 322,\n",
              " 'living': 323,\n",
              " 'money': 324,\n",
              " 'basic': 325,\n",
              " 'hard': 326,\n",
              " 'deficit': 327,\n",
              " 'prosperity': 328,\n",
              " 'allies': 329,\n",
              " 'insurance': 330,\n",
              " 'less': 331,\n",
              " 'three': 332,\n",
              " 'effective': 333,\n",
              " 'problem': 334,\n",
              " 'child': 335,\n",
              " 'done': 336,\n",
              " 'employment': 337,\n",
              " 'strengthen': 338,\n",
              " 'assistance': 339,\n",
              " 'purpose': 340,\n",
              " 'things': 341,\n",
              " 'place': 342,\n",
              " 'price': 343,\n",
              " 'second': 344,\n",
              " 'face': 345,\n",
              " 'democracy': 346,\n",
              " 'benefits': 347,\n",
              " 'then': 348,\n",
              " 'prices': 349,\n",
              " 'ahead': 350,\n",
              " 'live': 351,\n",
              " 'able': 352,\n",
              " 'real': 353,\n",
              " 'rate': 354,\n",
              " 'think': 355,\n",
              " 'half': 356,\n",
              " 'order': 357,\n",
              " 'total': 358,\n",
              " 'third': 359,\n",
              " 'laws': 360,\n",
              " 'farm': 361,\n",
              " 'communities': 362,\n",
              " 'always': 363,\n",
              " 'others': 364,\n",
              " 'seek': 365,\n",
              " 'old': 366,\n",
              " 'policies': 367,\n",
              " 'land': 368,\n",
              " 'course': 369,\n",
              " 'rates': 370,\n",
              " '4': 371,\n",
              " 'clear': 372,\n",
              " 'aid': 373,\n",
              " 'whole': 374,\n",
              " 'mr': 375,\n",
              " 'spirit': 376,\n",
              " 'bill': 377,\n",
              " 'needed': 378,\n",
              " '10': 379,\n",
              " 'measures': 380,\n",
              " 'investment': 381,\n",
              " 'political': 382,\n",
              " 'said': 383,\n",
              " 'industry': 384,\n",
              " 'general': 385,\n",
              " 'once': 386,\n",
              " 'become': 387,\n",
              " 'create': 388,\n",
              " 'faith': 389,\n",
              " 'higher': 390,\n",
              " 'achieve': 391,\n",
              " 'costs': 392,\n",
              " 'crime': 393,\n",
              " 'll': 394,\n",
              " 'nearly': 395,\n",
              " 'number': 396,\n",
              " 'four': 397,\n",
              " 'large': 398,\n",
              " 'improve': 399,\n",
              " 'research': 400,\n",
              " 'man': 401,\n",
              " 'move': 402,\n",
              " 'look': 403,\n",
              " 'services': 404,\n",
              " 'task': 405,\n",
              " 'few': 406,\n",
              " 'why': 407,\n",
              " 'food': 408,\n",
              " 'house': 409,\n",
              " 'stand': 410,\n",
              " 'certain': 411,\n",
              " 'maintain': 412,\n",
              " 'fight': 413,\n",
              " 'middle': 414,\n",
              " 'set': 415,\n",
              " 'democratic': 416,\n",
              " 'sure': 417,\n",
              " 'agreement': 418,\n",
              " 'times': 419,\n",
              " 'individual': 420,\n",
              " 'growing': 421,\n",
              " 'months': 422,\n",
              " 'm': 423,\n",
              " 'share': 424,\n",
              " 'leaders': 425,\n",
              " 'based': 426,\n",
              " 'ourselves': 427,\n",
              " 'recommend': 428,\n",
              " 'including': 429,\n",
              " 'level': 430,\n",
              " 'medical': 431,\n",
              " 'building': 432,\n",
              " 'office': 433,\n",
              " 'going': 434,\n",
              " 'find': 435,\n",
              " 'vital': 436,\n",
              " 'taken': 437,\n",
              " 'reduction': 438,\n",
              " 'east': 439,\n",
              " 'credit': 440,\n",
              " 'period': 441,\n",
              " 'unemployment': 442,\n",
              " 'debt': 443,\n",
              " 'abroad': 444,\n",
              " 'did': 445,\n",
              " 'young': 446,\n",
              " 'technology': 447,\n",
              " 'self': 448,\n",
              " 'does': 449,\n",
              " 'interests': 450,\n",
              " 'additional': 451,\n",
              " 'commission': 452,\n",
              " 'secure': 453,\n",
              " 'pass': 454,\n",
              " 'don': 455,\n",
              " 'justice': 456,\n",
              " 'opportunities': 457,\n",
              " 'themselves': 458,\n",
              " 'however': 459,\n",
              " 'department': 460,\n",
              " 'leadership': 461,\n",
              " 'sound': 462,\n",
              " 'commitment': 463,\n",
              " 'encourage': 464,\n",
              " 'fact': 465,\n",
              " 'across': 466,\n",
              " 'continued': 467,\n",
              " 'fair': 468,\n",
              " 'domestic': 469,\n",
              " 'executive': 470,\n",
              " 'attack': 471,\n",
              " '\\x95': 472,\n",
              " 'off': 473,\n",
              " 'urge': 474,\n",
              " 'recommendations': 475,\n",
              " 'start': 476,\n",
              " 'given': 477,\n",
              " 'proposed': 478,\n",
              " 'oil': 479,\n",
              " 'businesses': 480,\n",
              " 'lead': 481,\n",
              " 'capital': 482,\n",
              " 'message': 483,\n",
              " 'thank': 484,\n",
              " 'her': 485,\n",
              " 'remain': 486,\n",
              " 'alone': 487,\n",
              " 'threat': 488,\n",
              " 'society': 489,\n",
              " 'enough': 490,\n",
              " 'financial': 491,\n",
              " 'days': 492,\n",
              " 'cause': 493,\n",
              " 'begin': 494,\n",
              " 'standards': 495,\n",
              " 'equal': 496,\n",
              " 'result': 497,\n",
              " 'fellow': 498,\n",
              " 'require': 499,\n",
              " 'therefore': 500,\n",
              " 'term': 501,\n",
              " 'balance': 502,\n",
              " 'almost': 503,\n",
              " 'develop': 504,\n",
              " 'u': 505,\n",
              " 'until': 506,\n",
              " 'success': 507,\n",
              " 'wage': 508,\n",
              " 'serve': 509,\n",
              " 'communist': 510,\n",
              " 'aggression': 511,\n",
              " 'armed': 512,\n",
              " 'governments': 513,\n",
              " 'values': 514,\n",
              " 'ways': 515,\n",
              " 'steps': 516,\n",
              " 'management': 517,\n",
              " 'confidence': 518,\n",
              " 'relief': 519,\n",
              " 'civil': 520,\n",
              " 'increasing': 521,\n",
              " 'open': 522,\n",
              " 'air': 523,\n",
              " 'relations': 524,\n",
              " 'report': 525,\n",
              " 'generation': 526,\n",
              " 'serious': 527,\n",
              " 'friends': 528,\n",
              " 'god': 529,\n",
              " 'adequate': 530,\n",
              " 'agencies': 531,\n",
              " 'save': 532,\n",
              " 'around': 533,\n",
              " 'parents': 534,\n",
              " 'iraq': 535,\n",
              " 'low': 536,\n",
              " 'authority': 537,\n",
              " 'farmers': 538,\n",
              " 'single': 539,\n",
              " 'goals': 540,\n",
              " 'expand': 541,\n",
              " 'construction': 542,\n",
              " 'age': 543,\n",
              " 'markets': 544,\n",
              " 'peaceful': 545,\n",
              " 'veterans': 546,\n",
              " 'throughout': 547,\n",
              " 'respect': 548,\n",
              " 'peoples': 549,\n",
              " 'changes': 550,\n",
              " 'greatest': 551,\n",
              " 'passed': 552,\n",
              " 'agriculture': 553,\n",
              " 'arms': 554,\n",
              " 'recovery': 555,\n",
              " 'created': 556,\n",
              " 'fully': 557,\n",
              " 'reach': 558,\n",
              " 'true': 559,\n",
              " 'college': 560,\n",
              " 'science': 561,\n",
              " 'hold': 562,\n",
              " 'asia': 563,\n",
              " 'sense': 564,\n",
              " 'industrial': 565,\n",
              " 'fighting': 566,\n",
              " 'funds': 567,\n",
              " 'conditions': 568,\n",
              " 'chance': 569,\n",
              " 'training': 570,\n",
              " 'principles': 571,\n",
              " 'capacity': 572,\n",
              " 'increases': 573,\n",
              " 'initiative': 574,\n",
              " 'stop': 575,\n",
              " 'proud': 576,\n",
              " 'call': 577,\n",
              " 'afford': 578,\n",
              " 'levels': 579,\n",
              " 'longer': 580,\n",
              " 'beginning': 581,\n",
              " 'cities': 582,\n",
              " 'prevent': 583,\n",
              " 'example': 584,\n",
              " 'independence': 585,\n",
              " 'proposals': 586,\n",
              " 'protection': 587,\n",
              " 'join': 588,\n",
              " '6': 589,\n",
              " 'point': 590,\n",
              " 'finally': 591,\n",
              " 'senate': 592,\n",
              " 'washington': 593,\n",
              " 'away': 594,\n",
              " 'institutions': 595,\n",
              " 'called': 596,\n",
              " 'used': 597,\n",
              " 'china': 598,\n",
              " 'continuing': 599,\n",
              " 'record': 600,\n",
              " 'speaker': 601,\n",
              " 'brought': 602,\n",
              " 'itself': 603,\n",
              " 'ability': 604,\n",
              " 'quality': 605,\n",
              " 'comprehensive': 606,\n",
              " 'stronger': 607,\n",
              " 'natural': 608,\n",
              " 'field': 609,\n",
              " 'water': 610,\n",
              " 'issue': 611,\n",
              " 'requires': 612,\n",
              " 'market': 613,\n",
              " 'established': 614,\n",
              " 'assure': 615,\n",
              " 'whether': 616,\n",
              " 'step': 617,\n",
              " 'projects': 618,\n",
              " 'process': 619,\n",
              " 'decade': 620,\n",
              " 'dollar': 621,\n",
              " 'safety': 622,\n",
              " 'beyond': 623,\n",
              " 'recent': 624,\n",
              " 'kind': 625,\n",
              " 'historic': 626,\n",
              " 'available': 627,\n",
              " 'doing': 628,\n",
              " 'promote': 629,\n",
              " 'space': 630,\n",
              " 'cuts': 631,\n",
              " 'savings': 632,\n",
              " 'recommended': 633,\n",
              " 'expansion': 634,\n",
              " 'balanced': 635,\n",
              " 'promise': 636,\n",
              " 'activities': 637,\n",
              " 'improved': 638,\n",
              " 'helped': 639,\n",
              " 'enterprise': 640,\n",
              " 'reduced': 641,\n",
              " 'organization': 642,\n",
              " 'clean': 643,\n",
              " 'personal': 644,\n",
              " 'poverty': 645,\n",
              " 'tell': 646,\n",
              " 'something': 647,\n",
              " 'behind': 648,\n",
              " 'helping': 649,\n",
              " 'strategic': 650,\n",
              " 'especially': 651,\n",
              " '30': 652,\n",
              " 'raise': 653,\n",
              " 'basis': 654,\n",
              " 'modern': 655,\n",
              " 'actions': 656,\n",
              " 'trust': 657,\n",
              " 'bipartisan': 658,\n",
              " 'medicare': 659,\n",
              " 'global': 660,\n",
              " 'lower': 661,\n",
              " 'current': 662,\n",
              " 'carry': 663,\n",
              " '1947': 664,\n",
              " 'different': 665,\n",
              " 'affairs': 666,\n",
              " 'moment': 667,\n",
              " 'turn': 668,\n",
              " 'ensure': 669,\n",
              " 'role': 670,\n",
              " 'homes': 671,\n",
              " 'transportation': 672,\n",
              " 'conservation': 673,\n",
              " 'crisis': 674,\n",
              " 'coming': 675,\n",
              " 'rising': 676,\n",
              " 'she': 677,\n",
              " 'enforcement': 678,\n",
              " 'matter': 679,\n",
              " 'worked': 680,\n",
              " 'liberty': 681,\n",
              " '50': 682,\n",
              " 'developing': 683,\n",
              " 'fear': 684,\n",
              " 'area': 685,\n",
              " 'provided': 686,\n",
              " 'courage': 687,\n",
              " 'challenges': 688,\n",
              " 'recognize': 689,\n",
              " 'line': 690,\n",
              " 'several': 691,\n",
              " 'rural': 692,\n",
              " 'substantial': 693,\n",
              " 'environment': 694,\n",
              " 'difficult': 695,\n",
              " 'required': 696,\n",
              " 'korea': 697,\n",
              " 'took': 698,\n",
              " 'demand': 699,\n",
              " 'groups': 700,\n",
              " 'terrorists': 701,\n",
              " 'nothing': 702,\n",
              " 'earth': 703,\n",
              " 'danger': 704,\n",
              " 'immediate': 705,\n",
              " 'whose': 706,\n",
              " 'little': 707,\n",
              " 'teachers': 708,\n",
              " 'plans': 709,\n",
              " 'mean': 710,\n",
              " 'burden': 711,\n",
              " 'send': 712,\n",
              " 'coverage': 713,\n",
              " 'chamber': 714,\n",
              " '1980': 715,\n",
              " 'above': 716,\n",
              " 'five': 717,\n",
              " 'thing': 718,\n",
              " 'africa': 719,\n",
              " 'short': 720,\n",
              " 'providing': 721,\n",
              " 'agricultural': 722,\n",
              " 'existing': 723,\n",
              " 'choose': 724,\n",
              " 'students': 725,\n",
              " 'ready': 726,\n",
              " 'advance': 727,\n",
              " 'rest': 728,\n",
              " 'often': 729,\n",
              " 'facilities': 730,\n",
              " 'concern': 731,\n",
              " 'honor': 732,\n",
              " 'funding': 733,\n",
              " 'north': 734,\n",
              " 'side': 735,\n",
              " 'found': 736,\n",
              " 'begun': 737,\n",
              " 'permanent': 738,\n",
              " 'designed': 739,\n",
              " 'works': 740,\n",
              " 'might': 741,\n",
              " 'south': 742,\n",
              " 'vietnam': 743,\n",
              " 'speak': 744,\n",
              " 'along': 745,\n",
              " 'population': 746,\n",
              " 'purposes': 747,\n",
              " 'thousands': 748,\n",
              " 'deal': 749,\n",
              " 'remember': 750,\n",
              " 'month': 751,\n",
              " '20': 752,\n",
              " 'objectives': 753,\n",
              " 'nor': 754,\n",
              " 'surplus': 755,\n",
              " 'payments': 756,\n",
              " 'return': 757,\n",
              " 'competition': 758,\n",
              " 'safe': 759,\n",
              " 'preserve': 760,\n",
              " 'left': 761,\n",
              " 'industries': 762,\n",
              " 'wages': 763,\n",
              " '15': 764,\n",
              " 'determination': 765,\n",
              " 'approach': 766,\n",
              " 'citizen': 767,\n",
              " 'enemy': 768,\n",
              " 'goods': 769,\n",
              " '7': 770,\n",
              " 'addition': 771,\n",
              " 'entire': 772,\n",
              " 'built': 773,\n",
              " 'waste': 774,\n",
              " 'him': 775,\n",
              " 'information': 776,\n",
              " 'early': 777,\n",
              " 'drugs': 778,\n",
              " 'partnership': 779,\n",
              " 'troops': 780,\n",
              " 'duty': 781,\n",
              " 'produce': 782,\n",
              " 'responsibilities': 783,\n",
              " 'systems': 784,\n",
              " 'expect': 785,\n",
              " 'unity': 786,\n",
              " 'demands': 787,\n",
              " 'grow': 788,\n",
              " 'decisions': 789,\n",
              " 'met': 790,\n",
              " 'remains': 791,\n",
              " 'differences': 792,\n",
              " 'words': 793,\n",
              " 'afghanistan': 794,\n",
              " 'terror': 795,\n",
              " 'fund': 796,\n",
              " 'minimum': 797,\n",
              " 'include': 798,\n",
              " 'vote': 799,\n",
              " 'achieved': 800,\n",
              " 'position': 801,\n",
              " 'session': 802,\n",
              " 'treaty': 803,\n",
              " 'began': 804,\n",
              " 'agreements': 805,\n",
              " 'victory': 806,\n",
              " 'unless': 807,\n",
              " 'complete': 808,\n",
              " 'determined': 809,\n",
              " '21st': 810,\n",
              " 'held': 811,\n",
              " 'test': 812,\n",
              " 'products': 813,\n",
              " 'seen': 814,\n",
              " '8': 815,\n",
              " 'soon': 816,\n",
              " 'particularly': 817,\n",
              " 'meeting': 818,\n",
              " 'range': 819,\n",
              " 'asked': 820,\n",
              " 'poor': 821,\n",
              " 'instead': 822,\n",
              " 'skills': 823,\n",
              " 'big': 824,\n",
              " 'won': 825,\n",
              " 'stability': 826,\n",
              " 'race': 827,\n",
              " 'question': 828,\n",
              " 'supply': 829,\n",
              " 'situation': 830,\n",
              " 'prepared': 831,\n",
              " '12': 832,\n",
              " '1946': 833,\n",
              " 'measure': 834,\n",
              " 'responsible': 835,\n",
              " 'party': 836,\n",
              " 'taking': 837,\n",
              " 'choice': 838,\n",
              " 'army': 839,\n",
              " 'foundation': 840,\n",
              " 'estimated': 841,\n",
              " 'appropriations': 842,\n",
              " 'least': 843,\n",
              " 'leave': 844,\n",
              " 'hands': 845,\n",
              " 'incentives': 846,\n",
              " 'city': 847,\n",
              " 'objective': 848,\n",
              " 'productive': 849,\n",
              " 'importance': 850,\n",
              " 'thus': 851,\n",
              " 'atomic': 852,\n",
              " 'defend': 853,\n",
              " 'highest': 854,\n",
              " 'hand': 855,\n",
              " 'extend': 856,\n",
              " 'proposal': 857,\n",
              " 'd': 858,\n",
              " 'drug': 859,\n",
              " 'experience': 860,\n",
              " 'vast': 861,\n",
              " 'legislative': 862,\n",
              " 'fundamental': 863,\n",
              " 'recession': 864,\n",
              " 'region': 865,\n",
              " 'struggle': 866,\n",
              " 'mutual': 867,\n",
              " 'successful': 868,\n",
              " 'vice': 869,\n",
              " 'parties': 870,\n",
              " 'critical': 871,\n",
              " 'study': 872,\n",
              " '100': 873,\n",
              " 'majority': 874,\n",
              " 'japan': 875,\n",
              " 'powers': 876,\n",
              " '1945': 877,\n",
              " 'enemies': 878,\n",
              " 'conflict': 879,\n",
              " 'expanding': 880,\n",
              " 'address': 881,\n",
              " 'loans': 882,\n",
              " 'laughter': 883,\n",
              " 'companies': 884,\n",
              " 'access': 885,\n",
              " 'makes': 886,\n",
              " 'materials': 887,\n",
              " 'ii': 888,\n",
              " 'neighbors': 889,\n",
              " 'restore': 890,\n",
              " 'lost': 891,\n",
              " 'powerful': 892,\n",
              " 'group': 893,\n",
              " 'western': 894,\n",
              " 'willing': 895,\n",
              " 'retirement': 896,\n",
              " 'emergency': 897,\n",
              " 'provides': 898,\n",
              " 'stable': 899,\n",
              " 'whatever': 900,\n",
              " 'constitution': 901,\n",
              " 'show': 902,\n",
              " 'giving': 903,\n",
              " 'understanding': 904,\n",
              " 'expanded': 905,\n",
              " 'competitive': 906,\n",
              " 'either': 907,\n",
              " 'republic': 908,\n",
              " 'methods': 909,\n",
              " 'reforms': 910,\n",
              " 'permit': 911,\n",
              " 'moral': 912,\n",
              " 'amount': 913,\n",
              " 'peacetime': 914,\n",
              " '25': 915,\n",
              " 'average': 916,\n",
              " 'asking': 917,\n",
              " 'planning': 918,\n",
              " 'idea': 919,\n",
              " 'simple': 920,\n",
              " 'ours': 921,\n",
              " 'principle': 922,\n",
              " 'grants': 923,\n",
              " 'class': 924,\n",
              " 'got': 925,\n",
              " 'violence': 926,\n",
              " 'moving': 927,\n",
              " 'strategy': 928,\n",
              " '500': 929,\n",
              " 'tomorrow': 930,\n",
              " 'efficiency': 931,\n",
              " 'rise': 932,\n",
              " 'form': 933,\n",
              " 'per': 934,\n",
              " 'european': 935,\n",
              " 'run': 936,\n",
              " 'secretary': 937,\n",
              " 'understand': 938,\n",
              " 'sector': 939,\n",
              " 'creating': 940,\n",
              " 'heart': 941,\n",
              " 'issues': 942,\n",
              " 'efficient': 943,\n",
              " 'bank': 944,\n",
              " 'win': 945,\n",
              " 'improvement': 946,\n",
              " 'significant': 947,\n",
              " 'invest': 948,\n",
              " 'later': 949,\n",
              " 'offer': 950,\n",
              " 'everything': 951,\n",
              " 'ideals': 952,\n",
              " 'deficits': 953,\n",
              " 'value': 954,\n",
              " 'revolution': 955,\n",
              " 'succeed': 956,\n",
              " 'clearly': 957,\n",
              " 'steady': 958,\n",
              " 'attention': 959,\n",
              " 'consider': 960,\n",
              " 'assist': 961,\n",
              " 'individuals': 962,\n",
              " 'expected': 963,\n",
              " 'eliminate': 964,\n",
              " 'came': 965,\n",
              " 'employees': 966,\n",
              " 'comes': 967,\n",
              " 'speed': 968,\n",
              " 'police': 969,\n",
              " 'depends': 970,\n",
              " 'truly': 971,\n",
              " 'places': 972,\n",
              " 'june': 973,\n",
              " 'hemisphere': 974,\n",
              " 'committed': 975,\n",
              " 'key': 976,\n",
              " 'era': 977,\n",
              " 'directly': 978,\n",
              " '40': 979,\n",
              " 'particular': 980,\n",
              " 'final': 981,\n",
              " 'structure': 982,\n",
              " 'supplies': 983,\n",
              " 'direction': 984,\n",
              " 'realize': 985,\n",
              " 'spend': 986,\n",
              " 'simply': 987,\n",
              " 'lines': 988,\n",
              " 'close': 989,\n",
              " 'try': 990,\n",
              " 'investments': 991,\n",
              " 'broad': 992,\n",
              " 'rapidly': 993,\n",
              " 'regulations': 994,\n",
              " 'equipment': 995,\n",
              " 'decades': 996,\n",
              " 'establish': 997,\n",
              " 'results': 998,\n",
              " 'lot': 999,\n",
              " 'white': 1000,\n",
              " ...}"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 15,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the tokenizer knows what tokens (words) are in our corpus and has mapped them to numbers. The `keras` tokenizer also indexes them in order of frequency (most common first, i.e. index 1 is usually a word like \"the\"), which will come in handy later.\n",
        "\nAt this point, let's define the dimensions of our embeddings. It's up to you and your task to choose this number. Like many neural network hyperparameters, you may just need to play around with it."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 256"
      ],
      "outputs": [],
      "execution_count": 16,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's define the model. When I described the skip-gram task, I mentioned two inputs: the target word (also called the \"pivot\") and the context. So we're going to build two separate models for each input and then merge them into one."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "pivot_model = Sequential()\n",
        "pivot_model.add(Embedding(vocab_size, embedding_dim, input_length=1))\n",
        "\n",
        "context_model = Sequential()\n",
        "context_model.add(Embedding(vocab_size, embedding_dim, input_length=1))\n",
        "\n",
        "# merge the pivot and context models\n",
        "model = Sequential()\n",
        "model.add(Input([pivot_model, context_model]))\n",
        "model.add(Dot(axes=2))\n",
        "#model.add(Merge([pivot_model, context_model], mode='dot', dot_axes=2))\n",
        "model.add(Flatten())\n",
        "\n",
        "# the task as we've framed it here is\n",
        "# just binary classification,\n",
        "# so we want the output to be in [0,1],\n",
        "# and we can use binary crossentropy as our loss\n",
        "model.add(Activation('sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy')"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Error converting shape to a TensorShape: int() argument must be a string, a bytes-like object or a number, not 'Sequential'.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mmake_shape\u001b[0;34m(v, arg_name)\u001b[0m\n\u001b[1;32m    140\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36mas_shape\u001b[0;34m(shape)\u001b[0m\n\u001b[1;32m    945\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dims)\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;31m# Got a list of dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mas_dimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdims_iter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ndims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;31m# Got a list of dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mas_dimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdims_iter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ndims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36mas_dimension\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    481\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m       if (not isinstance(value, compat.bytes_or_text_types) and\n",
            "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'Sequential'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-5b7f70faebcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# merge the pivot and context models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpivot_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_model\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#model.add(Merge([pivot_model, context_model], mode='dot', dot_axes=2))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/input_layer.py\u001b[0m in \u001b[0;36mInput\u001b[0;34m(shape, batch_shape, name, dtype, sparse, tensor)\u001b[0m\n\u001b[1;32m    176\u001b[0m                              \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                              \u001b[0msparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                              input_tensor=tensor)\n\u001b[0m\u001b[1;32m    179\u001b[0m     \u001b[0;31m# Return tensor including _keras_shape and _keras_history.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;31m# Note that in this case train_output and test_output are the same pointer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/input_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_shape, batch_size, batch_input_shape, dtype, input_tensor, sparse, name)\u001b[0m\n\u001b[1;32m     85\u001b[0m                                          \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                                          \u001b[0msparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                                          name=self.name)\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_placeholder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mplaceholder\u001b[0;34m(shape, ndim, dtype, sparse, name)\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_placeholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_learning_phase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mplaceholder\u001b[0;34m(dtype, shape, name)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                        \"eager execution.\")\n\u001b[1;32m   1734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1736\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mplaceholder\u001b[0;34m(dtype, shape, name)\u001b[0m\n\u001b[1;32m   4921\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4922\u001b[0m       \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4923\u001b[0;31m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"shape\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4924\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m   4925\u001b[0m         \"Placeholder\", dtype=dtype, shape=shape, name=name)\n",
            "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mmake_shape\u001b[0;34m(v, arg_name)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error converting %s to a TensorShape: %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0marg_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     raise ValueError(\"Error converting %s to a TensorShape: %s.\" % (arg_name,\n",
            "\u001b[0;31mTypeError\u001b[0m: Error converting shape to a TensorShape: int() argument must be a string, a bytes-like object or a number, not 'Sequential'."
          ]
        }
      ],
      "execution_count": 31,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can train the model."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 60\n",
        "\n",
        "# used to sample words (indices)\n",
        "sampling_table = make_sampling_table(vocab_size)\n",
        "\n",
        "for i in range(n_epochs):\n",
        "    loss = 0\n",
        "    for seq in tokenizer.texts_to_sequences_generator(text_generator()):\n",
        "        # generate skip-gram training examples\n",
        "        # - `couples` consists of the pivots (i.e. target words) and surrounding contexts\n",
        "        # - `labels` represent if the context is true or not\n",
        "        # - `window_size` determines how far to look between words\n",
        "        # - `negative_samples` specifies the ratio of negative couples\n",
        "        #    (i.e. couples where the context is false)\n",
        "        #    to generate with respect to the positive couples;\n",
        "        #    i.e. `negative_samples=4` means \"generate 4 times as many negative samples\"\n",
        "        couples, labels = skipgrams(seq, vocab_size, window_size=5, negative_samples=4, sampling_table=sampling_table)\n",
        "        if couples:\n",
        "            pivot, context = zip(*couples)\n",
        "            pivot = np.array(pivot, dtype='int32')\n",
        "            context = np.array(context, dtype='int32')\n",
        "            labels = np.array(labels, dtype='int32')\n",
        "            loss += model.train_on_batch([pivot, context], labels)\n",
        "    print('epoch %d, %0.02f'%(i, loss))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, 51.97\n",
            "epoch 1, 31.86\n",
            "epoch 2, 22.61\n",
            "epoch 3, 19.98\n",
            "epoch 4, 18.96\n",
            "epoch 5, 18.48\n",
            "epoch 6, 18.19\n",
            "epoch 7, 18.02\n",
            "epoch 8, 17.93\n",
            "epoch 9, 17.84\n",
            "epoch 10, 17.79\n",
            "epoch 11, 17.71\n",
            "epoch 12, 17.61\n",
            "epoch 13, 17.58\n",
            "epoch 14, 17.50\n",
            "epoch 15, 17.38\n",
            "epoch 16, 17.32\n",
            "epoch 17, 17.20\n",
            "epoch 18, 17.08\n",
            "epoch 19, 16.89\n",
            "epoch 20, 16.75\n",
            "epoch 21, 16.59\n",
            "epoch 22, 16.38\n",
            "epoch 23, 16.19\n",
            "epoch 24, 16.00\n",
            "epoch 25, 15.77\n",
            "epoch 26, 15.57\n",
            "epoch 27, 15.31\n",
            "epoch 28, 15.13\n",
            "epoch 29, 14.82\n",
            "epoch 30, 14.61\n",
            "epoch 31, 14.35\n",
            "epoch 32, 14.08\n",
            "epoch 33, 13.80\n",
            "epoch 34, 13.61\n",
            "epoch 35, 13.34\n",
            "epoch 36, 13.09\n",
            "epoch 37, 12.81\n",
            "epoch 38, 12.61\n",
            "epoch 39, 12.37\n",
            "epoch 40, 12.13\n",
            "epoch 41, 11.89\n",
            "epoch 42, 11.69\n",
            "epoch 43, 11.49\n",
            "epoch 44, 11.28\n",
            "epoch 45, 11.11\n",
            "epoch 46, 10.91\n",
            "epoch 47, 10.72\n",
            "epoch 48, 10.57\n",
            "epoch 49, 10.42\n",
            "epoch 50, 10.27\n",
            "epoch 51, 10.14\n",
            "epoch 52, 9.98\n",
            "epoch 53, 9.81\n",
            "epoch 54, 9.68\n",
            "epoch 55, 9.58\n",
            "epoch 56, 9.47\n",
            "epoch 57, 9.38\n",
            "epoch 58, 9.24\n",
            "epoch 59, 9.11\n"
          ]
        }
      ],
      "execution_count": 11,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "With any luck, the model should finish training without a hitch.\n",
        "\nNow we can extract the embeddings, which are just the weights of the pivot embedding layer:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = model.get_weights()[0]"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also want to set aside the tokenizer's word index for later use (so we can get indices for words) and also create a reverse word index (so we can get words from indices):"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = tokenizer.word_index\n",
        "reverse_word_index = {v: k for k, v in word_index.items()}"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's it for learning the embeddings. Now we can try using them.\n",
        "\n",
        "## Getting similar words\n",
        "\n",
        "Each word embedding is just a mapping of a word to some point in space. So if we want to find words similar to some target word, we literally just need to look at the closest embeddings to that target word's embedding.\n",
        "\n",
        "An example will make this clearer.\n",
        "\nFirst, let's write a simple function to retrieve an embedding for a word:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding(word):\n",
        "    idx = word_index[word]\n",
        "    # make it 2d\n",
        "    return embeddings[idx][:,np.newaxis].T"
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we can define a function to get a most similar word for an input word:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "ignore_n_most_common = 50\n",
        "\n",
        "def get_closest(word):\n",
        "    embedding = get_embedding(word)\n",
        "\n",
        "    # get the distance from the embedding\n",
        "    # to every other embedding\n",
        "    distances = cdist(embedding, embeddings)[0]\n",
        "\n",
        "    # pair each embedding index and its distance\n",
        "    distances = list(enumerate(distances))\n",
        "\n",
        "    # sort from closest to furthest\n",
        "    distances = sorted(distances, key=lambda d: d[1])\n",
        "\n",
        "    # skip the first one; it's the target word\n",
        "    for idx, dist in distances[1:]:\n",
        "        # ignore the n most common words;\n",
        "        # they can get in the way.\n",
        "        # because the tokenizer organized indices\n",
        "        # from most common to least, we can just do this\n",
        "        if idx > ignore_n_most_common:\n",
        "            return reverse_word_index[idx]"
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's give it a try (you may get different results):"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_closest('freedom'))\n",
        "print(get_closest('justice'))\n",
        "print(get_closest('america'))\n",
        "print(get_closest('citizens'))\n",
        "print(get_closest('citizen'))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "teleprompter\n",
            "infused\n",
            "nation\n",
            "americans\n",
            "portray\n"
          ]
        }
      ],
      "execution_count": 16,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the most part, we seem to be getting related words!\n",
        "\nNB: Here we computed distances to _every_ other embedding, which is far from ideal when dealing with really large vocabularies. `Gensim`'s [`Word2Vec`](https://radimrehurek.com/gensim/models/word2vec.html) class implements a `most_similar` method that uses an approximate, but much faster, method for finding similar words. You can import the embeddings learned here into that class:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.doc2vec import Word2Vec\n",
        "\n",
        "with open('embeddings.dat', 'w') as f:\n",
        "    f.write('{} {}'.format(vocab_size, embedding_dim))\n",
        "\n",
        "    for word, idx in word_index.items():\n",
        "        embedding = ' '.join(str(d) for d in embeddings[idx])\n",
        "        f.write('\\n{} {}'.format(word, embedding))\n",
        "\n",
        "w2v = Word2Vec.load_word2vec_format('embeddings.dat', binary=False)\n",
        "print(w2v.most_similar(positive=['freedom']))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(u'religion', 0.5424818992614746), (u'nicaraguan', 0.5365787148475647), (u'world', 0.5312928557395935), (u'indivisible', 0.5296944379806519), (u'teleprompter', 0.5262748003005981), (u'peace', 0.5234299898147583), (u'nourish', 0.5190684199333191), (u'free', 0.5122354030609131), (u'siege', 0.5078175067901611), (u'untrammeled', 0.5072057247161865)]\n"
          ]
        }
      ],
      "execution_count": 17,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## t-SNE\n",
        "\n",
        "t-SNE (\"t-Distributed Stochastic Neighbor Embedding\") is a way of projecting high-dimensional data, e.g. our word embeddings, to a lower-dimension space, e.g. 2D, so we can visualize it.\n",
        "\n",
        "This will give us a better sense of the quality of our embeddings: we should see clusters of related words.\n",
        "\n`scikit-learn` provides a t-SNE implementation that is very easy to use."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# `n_components` is the number of dimensions to reduce to\n",
        "tsne = TSNE(n_components=2)\n",
        "\n",
        "# apply the dimensionality reduction\n",
        "# to our embeddings to get our 2d points\n",
        "points = tsne.fit_transform(embeddings)"
      ],
      "outputs": [],
      "execution_count": 19,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now let's plot it out:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(points)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.03630654  5.26957946]\n",
            " [ 0.01452018  5.46890038]\n",
            " [ 0.0155592   5.48057905]\n",
            " ..., \n",
            " [ 1.22333404 -1.02130924]\n",
            " [ 1.09427517  1.97301989]\n",
            " [ 0.27764715  0.70961465]]\n"
          ]
        }
      ],
      "execution_count": 20,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "matplotlib.use('Agg') # for pngs\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# plot our results\n",
        "# make it quite big so we can see everything\n",
        "fig, ax = plt.subplots(figsize=(40, 20))\n",
        "\n",
        "# extract x and y values separately\n",
        "xs = points[:,0]\n",
        "ys = points[:,1]\n",
        "\n",
        "# plot the points\n",
        "# we don't actually care about the point markers,\n",
        "# just want to automatically set the bounds of the plot\n",
        "ax.scatter(xs, ys, alpha=0)\n",
        "\n",
        "# annotate each point with its word\n",
        "for i, point in enumerate(points):\n",
        "    ax.annotate(reverse_word_index.get(i),\n",
        "                (xs[i], ys[i]),\n",
        "                fontsize=8)\n",
        "\nplt.savefig('tsne.png')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![tSNE of State of the Union word embeddings](../assets/tsne.png)\n",
        "\n",
        "This looks pretty good! It could certainly be improved upon, with more data or more training, but it's a great start.\n",
        "\n",
        "## Further Reading\n",
        "\n",
        "- [Deep Learning, NLP, and Representations](http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/). Chris Olah.\n",
        "- [On Word Embeddings](http://sebastianruder.com/word-embeddings-1/). Sebastian Ruder.\n",
        "- Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). [Efficient estimation of word representations in vector space](http://arxiv.org/pdf/1301.3781.pdf). arXiv preprint arXiv:1301.3781.\n",
        "- Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). [Distributed representations of words and phrases and their compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf). In Advances in neural information processing systems (pp. 3111-3119)."
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "toc": {
      "toc_position": {},
      "skip_h1_title": false,
      "number_sections": true,
      "title_cell": "Table of Contents",
      "toc_window_display": false,
      "base_numbering": 1,
      "toc_section_display": true,
      "title_sidebar": "Contents",
      "toc_cell": false,
      "nav_menu": {},
      "sideBar": true
    },
    "kernel_info": {
      "name": "python3"
    },
    "nteract": {
      "version": "0.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}