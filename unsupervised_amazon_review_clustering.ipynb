{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "unsupervised-amazon-review-clustering.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pgurazada/fast-fast-ai/blob/master/unsupervised_amazon_review_clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "M9ozwtlDuKMF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install contractions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Uo1hGKeJvOsI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/facebookresearch/fastText.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jn1Dey3P4w6j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%cd fastText"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ftMFP-Zj43hX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!make"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S-JQqK3m62hu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qVKJGsfks4LM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7HcJZkBWto9S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import re, string, unicodedata\n",
        "import nltk\n",
        "import contractions\n",
        "import inflect\n",
        "\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
        "\n",
        "from fastText import load_model\n",
        "\n",
        "from sklearn.cluster import SpectralClustering\n",
        "from sklearn.feature_extraction import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ptBQh4rDshIw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z7zaz5YVsqpB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HvxzaWR0zndr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K3xNSihkssIZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_df = pd.read_csv('/content/gdrive/My Drive/data-for-experimentation/amazon-reviews-corpus.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DBuiAiSJtCtQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "samples = data_df['text'].apply(str).values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3TNndjw3vZsm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "p = inflect.engine()\n",
        "stemmer = LancasterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ujzI81H_tNjC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cleaned_samples = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Cy_l3JE1vCYc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for sample in tqdm(samples):\n",
        "  cleaned_sample = []\n",
        "  \n",
        "  words = nltk.word_tokenize(sample)\n",
        "  \n",
        "  for word in words:\n",
        "    word = re.sub('\\[[^]]*\\]', '', word.lower())\n",
        "    word = re.sub(r'[^\\w\\s]', '', word)\n",
        "    word = contractions.fix(word)\n",
        "    \n",
        "    if word.isdigit():\n",
        "      word = p.number_to_words(word)\n",
        "    \n",
        "    word = stemmer.stem(word)\n",
        "    word = lemmatizer.lemmatize(word, pos='v')\n",
        "      \n",
        "    if word and word not in stopwords.words('english'):\n",
        "      cleaned_sample.append(word)\n",
        "      \n",
        "  cleaned_samples.append(' '.join(cleaned_sample))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KbScnhq611qQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open('/content/gdrive/My Drive/data-for-experimentation/amazon-reviews-cleaned.txt', 'w') as f:\n",
        "    for sample in cleaned_samples:\n",
        "        f.write(\"%s\\n\" % sample)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z9aBKS465F7i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! ./fasttext skipgram \\\n",
        "-input /content/gdrive/My\\ Drive/data-for-experimentation/amazon-reviews-cleaned.txt \\\n",
        "-output /content/gdrive/My\\ Drive/data-for-experimentation/amazon-skipgram-model \\\n",
        "-epoch 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GO2NgGhY5eXo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embedding_model = load_model('/content/gdrive/My Drive/data-for-experimentation/amazon-skipgram-model.bin')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H19LxlVk9Zhh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "words, embeddings = [], []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CDYgYqSa8wYc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for word in tqdm(embedding_model.get_words()):\n",
        "  words.append(word)\n",
        "  embeddings.append(embedding_model.get_word_vector(word))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OXUcIeVe9qTf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embeddings = np.array(embeddings, dtype='float')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cg_k2dYk92kl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cluster_model = SpectralClustering(assign_labels='discretize', random_state=20130810, n_jobs=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ICHLzluu-WDq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cluster_model.fit(embeddings)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3bNRjMCX_uaU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yWuuLTXL_u68",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Does clustering tf-idf important words improve clusters?"
      ]
    },
    {
      "metadata": {
        "id": "nvnK_R9--ZZA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cv = CountVectorizer(max_df=0.85, stop_words=stopwords.words('english'))\n",
        "tfidf_transformer = TfidfTransformer(smooth_idf=True, use_idf=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "etFy55ng_rcP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "word_count_vector = cv.fit_transform(cleaned_samples)\n",
        "tfidf_transformer.fit(word_count_vector)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}